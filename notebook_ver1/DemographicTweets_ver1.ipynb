{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Dataset\n",
    "\n",
    "In this notebook, we apply the back-door adjustment method to a dataset extracted from Twitter. Each user has been annotated with its location (NY or LA) and its gender (Male or Female).\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "1. [Load data from disk][1]\n",
    "  1. [Download data from Dropbox][2]\n",
    "  2. [Load data (functions)][3]\n",
    "  3. [Load data (functions calls)][4]\n",
    "    1. [Use gender as a confounder][5]\n",
    "    2. [Use location as a confounder][6]\n",
    "2. [Experiments][7]\n",
    "  1. [Load models][8]\n",
    "  2. [Experiment helper functions][9]\n",
    "  3. [Predict location with gender as a confounder][10]\n",
    "    1. [Accuracy experiment][11]\n",
    "    2. [Export figures][12]\n",
    "    3. [Simpson's paradox][13]\n",
    "    4. [Most changing features][14]\n",
    "    5. [Study effect of C on accuracy][15]\n",
    "    6. [Top terms table][16]\n",
    "  4. [Predict gender with location as a confounder][17]\n",
    "    1. [Accuracy experiment][18]\n",
    "    2. [Most changing features][19]\n",
    "    3. [Top terms table][20]\n",
    "[1]: #Load-data-from-disk\n",
    "[2]: #Download-data-from-Dropbox\n",
    "[3]: #Load-data-(functions)\n",
    "[4]: #Load-data-(functions-calls)\n",
    "[5]: #Use-gender-as-a-confounder\n",
    "[6]: #Use-location-as-a-confounder\n",
    "[7]: #Experiments\n",
    "[8]: #Load-models\n",
    "[9]: #Experiment-helper-functions\n",
    "[10]: #Predict-location-with-gender-as-a-confounder\n",
    "[11]: #Accuracy-experiment\n",
    "[12]: #Export-figures\n",
    "[13]: #Simpson's-paradox\n",
    "[14]: #Most-changing-features\n",
    "[15]: #Study-effect-of-C-on-accuracy\n",
    "[16]: #Top-terms-table\n",
    "[17]: #Predict-gender-with-location-as-a-confounder\n",
    "[18]: #Accuracy-experiment\n",
    "[19]: #Most-changing-features\n",
    "[20]: #Top-terms-table                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import os.path\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from disk\n",
    "\n",
    "### Download data from Dropbox\n",
    "The process of creating the dataset from 6000 users takes ~2 hours. Therefore, the results have been pickled and can be reloaded in ordered to get the dataset. Our data is [stored on Dropbox](https://www.dropbox.com/sh/pcg731664f8h4fy/AAD9GSey11NGJjgIgXsm5Mw9a/twitter.tgz?dl=1) and its access is protected by a password. If you are interested in using the data for research purposes, please email one of the author to obtain the password. Once you have downloaded and unpacked the data, set the following `DATAPATH` variable to the path where the data is stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to the data\n",
    "DATAPATH = '/Users/ustctll/Desktop/dataset'\n",
    "TWITTER_PATH = os.path.join(DATAPATH, 'twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    pass\n",
    "\n",
    "def load_dataset(users_pkl, term_doc_matrix_pkl, vectorizer_pkl=None, confounder_key='gender',\n",
    "                 train_ratio=.5, rand=np.random.RandomState(111191)):\n",
    "    data = Data()\n",
    "    if vectorizer_pkl is not None:\n",
    "        print(\"Loading feature names through vectorizer...\")\n",
    "        with open(vectorizer_pkl, 'rb') as f:\n",
    "            vec = pickle.load(f)\n",
    "        data.feature_names = np.array(vec.get_feature_names())[:20]\n",
    "    print(\"Loading users pickle...\")\n",
    "    with open(users_pkl, 'rb') as f:\n",
    "        all_users = pickle.load(f)\n",
    "    print(\"Load term document matrix pickle...\")\n",
    "    with open(term_doc_matrix_pkl, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    print(\"Getting label and confounder for every user...\")\n",
    "    Y = []\n",
    "    C = []\n",
    "    label_key = 'location' if confounder_key == 'gender' else 'gender'\n",
    "    for city_users in all_users:\n",
    "        for u in city_users:\n",
    "            C.append(u[confounder_key])\n",
    "            Y.append(u[label_key])\n",
    "    print(\"Done\")\n",
    "    print(\"%d users, %d features\" % X.shape)\n",
    "    C_set = set(C)\n",
    "    Y_set = set(Y)\n",
    "    for dc in C_set:\n",
    "        for dy in Y_set:\n",
    "            print(\"\\tc = %s and y = %s: %d users\" % (dc, dy, len([i for i in range(len(Y)) if Y[i]==dy and C[i]==dc])))\n",
    "            \n",
    "    le_C = preprocessing.LabelEncoder()\n",
    "    le_Y = preprocessing.LabelEncoder()\n",
    "    C_int = le_C.fit_transform(C)\n",
    "    Y_int = le_Y.fit_transform(Y)\n",
    "    print(le_C.classes_)\n",
    "    print(le_Y.classes_)\n",
    "    \n",
    "    indices = list(range(X.shape[0]))\n",
    "    rand.shuffle(indices)\n",
    "    train_size = int(train_ratio * X.shape[0])\n",
    "    train_idx = indices[:train_size]\n",
    "    test_idx = indices[train_size:]\n",
    "    data.train_x = X[train_idx,:100]\n",
    "    data.test_x = X[test_idx,:100]\n",
    "    data.train_c = C_int[train_idx]\n",
    "    data.test_c = C_int[test_idx]\n",
    "    data.train_y = Y_int[train_idx]\n",
    "    data.test_y = Y_int[test_idx]\n",
    "    print(data.train_x)\n",
    "    print(data.train_y)\n",
    "    print('lengths: x_train %d y_train %d c_train %d' % (data.train_x.shape[0], data.train_y.shape[0], data.train_c.shape[0]))\n",
    "    print('train y distr', Counter(data.train_y), 'c distr', Counter(data.train_c))\n",
    "    print('train y and c distr', Counter(['y=%d,c=%d' % (y,c) for (y,c) in zip(data.train_y,data.train_c)]))\n",
    "    \n",
    "    print('lengths: x_test %d y_test %d c_test %d' % (data.test_x.shape[0], data.test_y.shape[0], data.test_c.shape[0]))\n",
    "    print('test y distr', Counter(data.test_y), 'c distr', Counter(data.test_c))\n",
    "    print('test y and c distr', Counter(['y=%d,c=%d' % (y,c) for (y,c) in zip(data.test_y,data.test_c)]))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data (functions calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use gender as a confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature names through vectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator CountVectorizer from version pre-0.18 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading users pickle...\n",
      "Load term document matrix pickle...\n",
      "Getting label and confounder for every user...\n",
      "Done\n",
      "6000 users, 21981 features\n",
      "\tc = m and y = ny: 1500 users\n",
      "\tc = m and y = la: 1500 users\n",
      "\tc = f and y = ny: 1500 users\n",
      "\tc = f and y = la: 1500 users\n",
      "['f' 'm']\n",
      "['la' 'ny']\n",
      "  (0, 15)\t1\n",
      "  (0, 88)\t1\n",
      "  (1, 65)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 26)\t1\n",
      "  (1, 87)\t1\n",
      "  (1, 59)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 75)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 16)\t1\n",
      "  (2, 86)\t1\n",
      "  (2, 54)\t1\n",
      "  (2, 96)\t1\n",
      "  (2, 91)\t1\n",
      "  (2, 47)\t1\n",
      "  (2, 70)\t1\n",
      "  (2, 99)\t1\n",
      "  (2, 33)\t1\n",
      "  (2, 65)\t1\n",
      "  (2, 56)\t1\n",
      "  (2, 78)\t1\n",
      "  (2, 14)\t1\n",
      "  :\t:\n",
      "  (2997, 6)\t1\n",
      "  (2997, 26)\t1\n",
      "  (2997, 23)\t1\n",
      "  (2997, 87)\t1\n",
      "  (2997, 60)\t1\n",
      "  (2997, 61)\t1\n",
      "  (2997, 48)\t1\n",
      "  (2997, 41)\t1\n",
      "  (2997, 11)\t1\n",
      "  (2997, 69)\t1\n",
      "  (2997, 24)\t1\n",
      "  (2997, 59)\t1\n",
      "  (2997, 1)\t1\n",
      "  (2997, 88)\t1\n",
      "  (2997, 75)\t1\n",
      "  (2997, 42)\t1\n",
      "  (2997, 0)\t1\n",
      "  (2997, 17)\t1\n",
      "  (2997, 44)\t1\n",
      "  (2997, 16)\t1\n",
      "  (2998, 12)\t1\n",
      "  (2998, 88)\t1\n",
      "  (2998, 0)\t1\n",
      "  (2998, 44)\t1\n",
      "  (2999, 16)\t1\n",
      "[1 1 1 ... 1 1 0]\n",
      "lengths: x_train 3000 y_train 3000 c_train 3000\n",
      "train y distr Counter({0: 1527, 1: 1473}) c distr Counter({1: 1511, 0: 1489})\n",
      "train y and c distr Counter({'y=0,c=1': 772, 'y=0,c=0': 755, 'y=1,c=1': 739, 'y=1,c=0': 734})\n",
      "lengths: x_test 3000 y_test 3000 c_test 3000\n",
      "test y distr Counter({1: 1527, 0: 1473}) c distr Counter({0: 1511, 1: 1489})\n",
      "test y and c distr Counter({'y=1,c=0': 766, 'y=1,c=1': 761, 'y=0,c=0': 745, 'y=0,c=1': 728})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(users_pkl=os.path.join(TWITTER_PATH, \"users_array.pkl\"),\n",
    "                    term_doc_matrix_pkl=os.path.join(TWITTER_PATH, \"term_doc_matrix.pkl\"),\n",
    "                    vectorizer_pkl=os.path.join(TWITTER_PATH, \"vectorizer.pkl\"),\n",
    "                    confounder_key='gender',\n",
    "                    train_ratio=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use location as a confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature names through vectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator CountVectorizer from version pre-0.18 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading users pickle...\n",
      "Load term document matrix pickle...\n",
      "Getting label and confounder for every user...\n",
      "Done\n",
      "6000 users, 21981 features\n",
      "\tc = ny and y = m: 1500 users\n",
      "\tc = ny and y = f: 1500 users\n",
      "\tc = la and y = m: 1500 users\n",
      "\tc = la and y = f: 1500 users\n",
      "['la' 'ny']\n",
      "['f' 'm']\n",
      "  (0, 55)\t1\n",
      "  (0, 66)\t1\n",
      "  (0, 86)\t1\n",
      "  (0, 84)\t1\n",
      "  (0, 83)\t1\n",
      "  (0, 96)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 78)\t1\n",
      "  (0, 81)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 82)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 87)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 69)\t1\n",
      "  (0, 59)\t1\n",
      "  :\t:\n",
      "  (2997, 16)\t1\n",
      "  (2998, 98)\t1\n",
      "  (2998, 68)\t1\n",
      "  (2998, 96)\t1\n",
      "  (2998, 14)\t1\n",
      "  (2998, 10)\t1\n",
      "  (2998, 5)\t1\n",
      "  (2998, 6)\t1\n",
      "  (2998, 60)\t1\n",
      "  (2998, 73)\t1\n",
      "  (2998, 11)\t1\n",
      "  (2998, 59)\t1\n",
      "  (2998, 1)\t1\n",
      "  (2998, 58)\t1\n",
      "  (2998, 88)\t1\n",
      "  (2998, 75)\t1\n",
      "  (2998, 74)\t1\n",
      "  (2998, 44)\t1\n",
      "  (2998, 16)\t1\n",
      "  (2999, 31)\t1\n",
      "  (2999, 56)\t1\n",
      "  (2999, 45)\t1\n",
      "  (2999, 1)\t1\n",
      "  (2999, 44)\t1\n",
      "  (2999, 16)\t1\n",
      "[0 1 0 ... 0 0 0]\n",
      "lengths: x_train 3000 y_train 3000 c_train 3000\n",
      "train y distr Counter({0: 1503, 1: 1497}) c distr Counter({0: 1501, 1: 1499})\n",
      "train y and c distr Counter({'y=0,c=1': 762, 'y=1,c=0': 760, 'y=0,c=0': 741, 'y=1,c=1': 737})\n",
      "lengths: x_test 3000 y_test 3000 c_test 3000\n",
      "test y distr Counter({1: 1503, 0: 1497}) c distr Counter({1: 1501, 0: 1499})\n",
      "test y and c distr Counter({'y=1,c=1': 763, 'y=0,c=0': 759, 'y=1,c=0': 740, 'y=0,c=1': 738})\n"
     ]
    }
   ],
   "source": [
    "data2 = load_dataset(users_pkl=os.path.join(TWITTER_PATH, \"users_array.pkl\"),\n",
    "                    term_doc_matrix_pkl=os.path.join(TWITTER_PATH, \"term_doc_matrix.pkl\"),\n",
    "                    vectorizer_pkl=os.path.join(TWITTER_PATH, \"vectorizer.pkl\"),\n",
    "                    confounder_key='location',\n",
    "                    train_ratio=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models.py\n",
    "%run injecting_bias.py\n",
    "%run confound_plot.py\n",
    "%run most_changing_coef.py\n",
    "%run ba_c_study.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backdoor_adjustment = lambda x,y,z,t,u: backdoor_adjustment_var_C(x,y,z,1.,t,u,)\n",
    "backdoor_adjustment_Z10 = lambda x,y,z,t,u: backdoor_adjustment_var_C(x,y,z,10,t,u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_confounding_trials(models, data, ntrials, rand):  \n",
    "    \"\"\" Do several random trials in which we sample data with a confounding variable. \n",
    "    Plot the average accuracies as confounding bias increases.\n",
    "    \"\"\"\n",
    "    test_biases = [.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "    train_biases = [.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "    #test_biases = [.5]\n",
    "    #train_biases = [.5]\n",
    "#     test_biases = [.1, .5, .9]\n",
    "#     train_biases = [.1, .5, .9]\n",
    "    corr_diffs = []\n",
    "    accuracies = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    \n",
    "    for train_bias in train_biases:\n",
    "        for test_bias in test_biases:\n",
    "            for ti in range(ntrials):\n",
    "                # Sample training and testing indices.\n",
    "                test_idx = make_confounding_data(X=data.test_x, y=data.test_y, c=data.test_c,\n",
    "                                                pos_prob=.5, bias=test_bias, size=800, rand=rand)  \n",
    "                test_corr = pearsonr(data.test_y[test_idx], data.test_c[test_idx])[0]\n",
    "                train_idx = make_confounding_data(X=data.train_x, y=data.train_y, c=data.train_c,\n",
    "                                                  pos_prob=.5, bias=train_bias, size=800, rand=rand)   \n",
    "                train_corr = pearsonr(data.train_y[train_idx], data.train_c[train_idx])[0]\n",
    "                corr_diff = round(train_corr - test_corr, 1)\n",
    "                if ti == 0:\n",
    "                    corr_diffs.append(corr_diff)\n",
    "                    print('train_bias=', train_bias, 'train_corr=', train_corr,\n",
    "                          'test_bias=', test_bias, 'test_corr=', test_corr,\n",
    "                          'corr_diff=', corr_diff)\n",
    "                    \n",
    "                # Train and test each model.\n",
    "                for name, model in models:\n",
    "                    clf = model(data.train_x[train_idx], data.train_y[train_idx],\n",
    "                                data.train_c[train_idx], rand, data.feature_names)\n",
    "                    y_pred = clf.predict(data.test_x[test_idx])\n",
    "                    y_true = data.test_y[test_idx]\n",
    "                    for y in range(3):\n",
    "                        for c in range(3):\n",
    "                            k = 3*y+c\n",
    "                            cond = lambda x: (c == 2 or data.test_c[x] == c) and (y == 2 or data.test_y[x] == y)\n",
    "                            yc_test_idx = [i for i, j in enumerate(test_idx) if cond(j)]\n",
    "                            accuracies[k][name].append({'test_bias': test_bias, 'train_bias': train_bias,\n",
    "                                                        'corr_diff': corr_diff,\n",
    "                                                        'acc': accuracy_score(y_true[yc_test_idx],\n",
    "                                                                              y_pred[yc_test_idx])})\n",
    "                            print('name',name,'acc',accuracy_score(y_true[yc_test_idx], y_pred[yc_test_idx]))\n",
    "                        \n",
    "    return accuracies, corr_diffs, test_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_confound_expt(data, ntrials=3, models=[('feature_select', feature_select),('logreg', lr),              \n",
    "                                             ('matching', matching),('backdoor_adjustment', backdoor_adjustment)],\n",
    "                    confounding_function=do_confounding_trials):                                              \n",
    "    rand = np.random.RandomState(1234567)                                                                     \n",
    "    clf = lr(data.train_x, data.train_y, data.train_c, rand, data.feature_names)                              \n",
    "    print('og testing accuracy=', accuracy_score(data.test_y, clf.predict(data.test_x)))                      \n",
    "    print('----------------\\nExperiments using genre as a confounder:')                                       \n",
    "    return confounding_function(models, data, ntrials, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict location with gender as a confounder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "og testing accuracy= 0.553\n",
      "----------------\n",
      "Experiments using genre as a confounder:\n",
      "train_bias= 0.1 train_corr= -0.8000100001875037 test_bias= 0.1 test_corr= -0.8 corr_diff= -0.0\n",
      "name LR acc 0.6\n",
      "name LR acc 0.5722222222222222\n",
      "name LR acc 0.575\n",
      "name LR acc 0.5277777777777778\n",
      "name LR acc 0.5\n",
      "name LR acc 0.525\n",
      "name LR acc 0.535\n",
      "name LR acc 0.565\n",
      "name LR acc 0.55\n",
      "name M acc 0.7\n",
      "name M acc 0.6055555555555555\n",
      "name M acc 0.615\n",
      "name M acc 0.55\n",
      "name M acc 0.35\n",
      "name M acc 0.53\n",
      "name M acc 0.565\n",
      "name M acc 0.58\n",
      "name M acc 0.5725\n",
      "name BA acc 0.575\n",
      "name BA acc 0.5333333333333333\n",
      "name BA acc 0.5375\n",
      "name BA acc 0.5361111111111111\n",
      "name BA acc 0.5\n",
      "name BA acc 0.5325\n",
      "name BA acc 0.54\n",
      "name BA acc 0.53\n",
      "name BA acc 0.535\n",
      "name SO acc 0.55\n",
      "name SO acc 0.5777777777777777\n",
      "name SO acc 0.575\n",
      "name SO acc 0.5388888888888889\n",
      "name SO acc 0.55\n",
      "name SO acc 0.54\n",
      "name SO acc 0.54\n",
      "name SO acc 0.575\n",
      "name SO acc 0.5575\n",
      "name LRS acc 0.725\n",
      "name LRS acc 0.575\n",
      "name LRS acc 0.59\n",
      "name LRS acc 0.4722222222222222\n",
      "name LRS acc 0.575\n",
      "name LRS acc 0.4825\n",
      "name LRS acc 0.4975\n",
      "name LRS acc 0.575\n",
      "name LRS acc 0.53625\n",
      "name BAZ10 acc 0.575\n",
      "name BAZ10 acc 0.5333333333333333\n",
      "name BAZ10 acc 0.5375\n",
      "name BAZ10 acc 0.5388888888888889\n",
      "name BAZ10 acc 0.525\n",
      "name BAZ10 acc 0.5375\n",
      "name BAZ10 acc 0.5425\n",
      "name BAZ10 acc 0.5325\n",
      "name BAZ10 acc 0.5375\n",
      "name CDT acc 0.35\n",
      "name CDT acc 0.3277777777777778\n",
      "name CDT acc 0.33\n",
      "name CDT acc 0.7472222222222222\n",
      "name CDT acc 0.775\n",
      "name CDT acc 0.75\n",
      "name CDT acc 0.7075\n",
      "name CDT acc 0.3725\n",
      "name CDT acc 0.54\n",
      "train_bias= 0.1 train_corr= -0.8000100001875037 test_bias= 0.2 test_corr= -0.6 corr_diff= -0.2\n",
      "name LR acc 0.525\n",
      "name LR acc 0.640625\n",
      "name LR acc 0.6175\n",
      "name LR acc 0.53125\n",
      "name LR acc 0.4125\n",
      "name LR acc 0.5075\n",
      "name LR acc 0.53\n",
      "name LR acc 0.595\n",
      "name LR acc 0.5625\n",
      "name M acc 0.4\n",
      "name M acc 0.459375\n",
      "name M acc 0.4475\n",
      "name M acc 0.60625\n",
      "name M acc 0.5375\n",
      "name M acc 0.5925\n",
      "name M acc 0.565\n",
      "name M acc 0.475\n",
      "name M acc 0.52\n",
      "name BA acc 0.6625\n",
      "name BA acc 0.64375\n",
      "name BA acc 0.6475\n",
      "name BA acc 0.44375\n",
      "name BA acc 0.3625\n",
      "name BA acc 0.4275\n",
      "name BA acc 0.4875\n",
      "name BA acc 0.5875\n",
      "name BA acc 0.5375\n",
      "name SO acc 0.5125\n",
      "name SO acc 0.659375\n",
      "name SO acc 0.63\n",
      "name SO acc 0.525\n",
      "name SO acc 0.4125\n",
      "name SO acc 0.5025\n",
      "name SO acc 0.5225\n",
      "name SO acc 0.61\n",
      "name SO acc 0.56625\n",
      "name LRS acc 0.6375\n",
      "name LRS acc 0.6125\n",
      "name LRS acc 0.6175\n",
      "name LRS acc 0.46875\n",
      "name LRS acc 0.3625\n",
      "name LRS acc 0.4475\n",
      "name LRS acc 0.5025\n",
      "name LRS acc 0.5625\n",
      "name LRS acc 0.5325\n",
      "name BAZ10 acc 0.675\n",
      "name BAZ10 acc 0.640625\n",
      "name BAZ10 acc 0.6475\n",
      "name BAZ10 acc 0.4375\n",
      "name BAZ10 acc 0.3375\n",
      "name BAZ10 acc 0.4175\n",
      "name BAZ10 acc 0.485\n",
      "name BAZ10 acc 0.58\n",
      "name BAZ10 acc 0.5325\n",
      "name CDT acc 0.6125\n",
      "name CDT acc 0.64375\n",
      "name CDT acc 0.6375\n",
      "name CDT acc 0.45625\n",
      "name CDT acc 0.275\n",
      "name CDT acc 0.42\n",
      "name CDT acc 0.4875\n",
      "name CDT acc 0.57\n",
      "name CDT acc 0.52875\n",
      "train_bias= 0.1 train_corr= -0.8000100001875037 test_bias= 0.3 test_corr= -0.4 corr_diff= -0.4\n",
      "name LR acc 0.5833333333333334\n",
      "name LR acc 0.6642857142857143\n",
      "name LR acc 0.64\n",
      "name LR acc 0.48214285714285715\n",
      "name LR acc 0.4583333333333333\n",
      "name LR acc 0.475\n",
      "name LR acc 0.5125\n",
      "name LR acc 0.6025\n",
      "name LR acc 0.5575\n",
      "name M acc 0.475\n",
      "name M acc 0.475\n",
      "name M acc 0.475\n",
      "name M acc 0.6642857142857143\n",
      "name M acc 0.6333333333333333\n",
      "name M acc 0.655\n",
      "name M acc 0.6075\n",
      "name M acc 0.5225\n",
      "name M acc 0.565\n",
      "name BA acc 0.575\n",
      "name BA acc 0.5678571428571428\n",
      "name BA acc 0.57\n",
      "name BA acc 0.475\n",
      "name BA acc 0.5166666666666667\n",
      "name BA acc 0.4875\n",
      "name BA acc 0.505\n",
      "name BA acc 0.5525\n",
      "name BA acc 0.52875\n",
      "name SO acc 0.575\n",
      "name SO acc 0.65\n",
      "name SO acc 0.6275\n",
      "name SO acc 0.5\n",
      "name SO acc 0.475\n",
      "name SO acc 0.4925\n",
      "name SO acc 0.5225\n",
      "name SO acc 0.5975\n",
      "name SO acc 0.56\n",
      "name LRS acc 0.49166666666666664\n",
      "name LRS acc 0.5178571428571429\n",
      "name LRS acc 0.51\n",
      "name LRS acc 0.5178571428571429\n",
      "name LRS acc 0.5583333333333333\n",
      "name LRS acc 0.53\n",
      "name LRS acc 0.51\n",
      "name LRS acc 0.53\n",
      "name LRS acc 0.52\n",
      "name BAZ10 acc 0.575\n",
      "name BAZ10 acc 0.5642857142857143\n",
      "name BAZ10 acc 0.5675\n",
      "name BAZ10 acc 0.48214285714285715\n",
      "name BAZ10 acc 0.5\n",
      "name BAZ10 acc 0.4875\n",
      "name BAZ10 acc 0.51\n",
      "name BAZ10 acc 0.545\n",
      "name BAZ10 acc 0.5275\n"
     ]
    }
   ],
   "source": [
    "models = [('LR', lr), ('M', matching), ('BA', backdoor_adjustment), ('SO', sumout), ('LRS', lr_subsampling),\n",
    "          ('BAZ10', backdoor_adjustment_Z10),('CDT',cdtAlg)]\n",
    "#models = [('CDT',cdtAlg)]\n",
    "accuracies, corr_diffs, test_biases = do_confound_expt(data2, ntrials=1, models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_bias_axis = (test_biases, 'test_bias', 'Test Bias')\n",
    "corr_diff_axis = (corr_diffs, 'corr_diff', 'correlation difference (train-test)')\n",
    "\n",
    "to_plot = ['logistic regression']\n",
    "plot_all_accuracies(accuracies, test_bias_axis, title='Average accuracy/Test bias', xlim=[0,1])#, keys=to_plot)\n",
    "plot_all_accuracies(accuracies, corr_diff_axis, title='Accuracy/Correlation difference')#, keys=to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range(3):\n",
    "    for c in range(3):\n",
    "        plot_accuracy(accuracies, test_bias_axis, y=y, c=c, xlim=[0,1])\n",
    "        plot_accuracy(accuracies, corr_diff_axis, y=y, c=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tr_bias in np.arange(0.1, 1., 0.1):\n",
    "    ylabel=\"Accuracy for train bias=%.1f\" % tr_bias\n",
    "    export_plot_accuracy('test', accuracies, test_bias_axis, 2, 2, title='', xlabel='Test bias', train_bias=tr_bias,\n",
    "                         ylabel=ylabel, xlim=[0.,1.], set_xticks=np.arange(0.1,1.,.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplot = ['BA', 'BAZ10', 'LR', 'M', 'LRS']\n",
    "\n",
    "# Export IMDb plots\n",
    "fig = export_plot_accuracy('../paper/figures/twitter_accuracy_corr_diff.pdf',\n",
    "                           accuracies, corr_diff_axis, 2, 2, title='',\n",
    "                           xlabel='Correlation difference (train-test)', ncol=3,\n",
    "                           ylabel='Accuracy')\n",
    "\n",
    "fig = export_plot_accuracy('../paper/figures/twitter_accuracy_test_bias.pdf',\n",
    "                           accuracies, test_bias_axis, 2, 2, title='', ncol=3,\n",
    "                           xlabel='Test bias',\n",
    "                           ylabel='Accuracy averaged over the training biases', xlim=[0.,1.], set_xticks=np.arange(0.1,1.,.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simpson's paradox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run simpson_paradox.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [('BAZ10', backdoor_adjustment_Z10), ('LR', lr), ('LRS', lr_subsampling), ('BA', backdoor_adjustment)]\n",
    "rand = np.random.RandomState(111191)\n",
    "biases, spa_count_bias_results = simpson_paradox_count_bias(data, methods, 800, rand=rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {\n",
    "    'BAZ10': 'gv-',\n",
    "    'LR': 'rx-',\n",
    "    'BA': 'bs-',\n",
    "    'LRS': 'cD-'\n",
    "}\n",
    "plot_spa_results(biases, spa_count_bias_results, markers, tofile=\"../paper/figures/simpson_paradox_expt.pdf\", n_fts=21981.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most changing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run most_changing_coef.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = dict(data=data,\n",
    "              n=30,\n",
    "              models=[(backdoor_adjustment, 'BA', 'bs'), (backdoor_adjustment_Z10, 'BAZ10', 'gv')],\n",
    "              biases=[.1,.5,.9],\n",
    "              size=800,\n",
    "              transformation=most_changing_coef,\n",
    "              class_labels=['Los Angeles', 'New York'],\n",
    "              feature_names=np.hstack([data.feature_names, ['c=0', 'c=1']]))\n",
    "changing_coef_plot(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run most_changing_coef.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x2, pval = chi2(data.train_x, data.train_c)\n",
    "top_ft_idx = np.argsort(x2)[::-1][:10][::-1]\n",
    "params = dict(data=data,\n",
    "              models=[(lr, 'LR', 'rx'), (backdoor_adjustment, 'BA', 'bs'), (backdoor_adjustment_Z10, 'BAZ10', 'gv')],\n",
    "              biases=[.9],\n",
    "              size=800,\n",
    "              trials=2,\n",
    "              class_labels=['Los Angeles', 'New York'],\n",
    "              indices=top_ft_idx)\n",
    "changing_coef_plot_given_idx(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x2, pval = chi2(data.train_x, data.train_y)\n",
    "top_ft_idx = np.argsort(x2)[::-1][:10][::-1]\n",
    "params = dict(data=data,\n",
    "              models=[(lr, 'LR', 'rx'), (backdoor_adjustment, 'BA', 'bs'), (backdoor_adjustment_Z10, 'BAZ10', 'gv')],\n",
    "              biases=[.9],\n",
    "              size=800,\n",
    "              trials=2,\n",
    "              class_labels=['Los Angeles', 'New York'],\n",
    "              indices=top_ft_idx)\n",
    "changing_coef_plot_given_idx(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study effect of C on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_range = np.logspace(-3, 4, 15)\n",
    "methods = [('BA', backdoor_adjustment_var_C)]\n",
    "filter_corr_diff = lambda x: np.abs(x) > 1.2\n",
    "accuracies_c, coefs_c = do_c_study(c_range, filter_corr_diff, data, 5,\n",
    "                                   np.random.RandomState(111191), 800, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_c_study(c_range, accuracies_c, coefs_c, tofile='../paper/figures/ba_c_study.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top terms table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run top_terms_table.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_top_coef_table(data, lr, 5, 800, 10, np.random.RandomState(111191), 1., data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_top_coef_table(data, backdoor_adjustment, 5, 800, 10, np.random.RandomState(111191), 1., data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_top_coef_table(data, backdoor_adjustment_Z10, 5, 800, 10, np.random.RandomState(111191), 1., data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict gender with location as a confounder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [('LR', lr), ('M', matching), ('BA', backdoor_adjustment), ('SO', sumout), ('LRS', lr_subsampling),\n",
    "          ('BAZ10', backdoor_adjustment_Z10)]\n",
    "accuracies2, corr_diffs2, test_biases2 = do_confound_expt(data2, ntrials=5, models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_bias_axis2 = (test_biases2, 'test_bias', 'Test Bias')\n",
    "corr_diff_axis2 = (corr_diffs2, 'corr_diff', 'correlation difference (train-test)')\n",
    "\n",
    "plot_all_accuracies(accuracies2, test_bias_axis2, title='Average accuracy/Test bias', xlim=[0,1])#, keys=to_plot)\n",
    "plot_all_accuracies(accuracies2, corr_diff_axis2, title='Accuracy/Correlation difference')#, keys=to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for y in range(3):\n",
    "    for c in range(3):\n",
    "        plot_accuracy(accuracies2, test_bias_axis2, y=y, c=c, xlim=[0,1])#, keys=['lr subsampling', 'backdoor adjustment'])\n",
    "        plot_accuracy(accuracies2, corr_diff_axis2, y=y, c=c)#, keys=['lr subsampling', 'backdoor adjustment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most changing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run most_changing_coef.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = dict(data=data2,\n",
    "              n=30,\n",
    "              models=[(backdoor_adjustment, 'BA', 'bs'),(backdoor_adjustment_Z10, 'BAZ10', 'gv')],\n",
    "              biases=[.1,.5,.9],\n",
    "              size=800,\n",
    "              transformation=most_changing_coef,\n",
    "              class_labels=['Female', 'Male'],\n",
    "              feature_names=np.hstack([data.feature_names, ['c=0', 'c=1']]))\n",
    "changing_coef_plot(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = dict(data=data2,\n",
    "              n=60,\n",
    "              models=[(lr, 'LR', 'rx'), (backdoor_adjustment_Z10, 'BAZ10', 'gv')],\n",
    "              biases=[.1,.5,.9],\n",
    "              size=800,\n",
    "              transformation=most_changing_coef,\n",
    "              class_labels=['Female', 'Male'])\n",
    "changing_coef_plot(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top terms table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run top_terms_table.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do_top_coef_table(data2, lr, 5, 800, 10, np.random.RandomState(111191), 1., data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_top_coef_table(data2, backdoor_adjustment, 5, 800, 10, np.random.RandomState(111191), 1., data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_top_coef_table(data2, backdoor_adjustment_Z10, 5, 800, 10, np.random.RandomState(111191), 1., data.feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
